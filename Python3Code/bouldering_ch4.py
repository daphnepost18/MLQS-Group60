##############################################################
#                                                            #
#    Mark Hoogendoorn and Burkhardt Funk (2017)              #
#    Machine Learning for the Quantified Self                #
#    Springer                                                #
#    Chapter 4                                               #
#                                                            #
##############################################################

import sys
import copy
import pandas as pd
import time
from pathlib import Path
import argparse

from util.VisualizeDataset import VisualizeDataset
from Chapter4.TemporalAbstraction import NumericalAbstraction
from Chapter4.TemporalAbstraction import CategoricalAbstraction
from Chapter4.FrequencyAbstraction import FourierTransformation
from Chapter4.TextAbstraction import TextAbstraction

# Read the result from the previous chapter
DATA_PATH = Path('./intermediate_datafiles_bouldering/')


def print_flags():
    """
    Prints all entries in FLAGS variable.
    """
    for key, value in vars(FLAGS).items():
        print(key + ' : ' + str(value))


def main():
    print_flags()

    start_time = time.time()

    # Get all chapter 3 final result files as input
    input_files = list(DATA_PATH.glob('chapter3_result_final_*.csv'))

    if not input_files:
        print("No Chapter 3 final result files found. Please run bouldering_ch3_rest.py with --mode final first.")
        return

    NumAbs = NumericalAbstraction()
    FreqAbs = FourierTransformation()

    # Define the consistent column names from the bouldering datasets
    ACC_X = 'acc_X (m/s^2)'
    GYR_X = 'gyr_X (rad/s)'
    MAG_X = 'mag_X (µT)'
    LOC_HEIGHT = 'loc_Height (m)'
    PCA_1 = 'pca_1' # This column is generated by PCA in bouldering_ch3_rest.py

    for input_file_path in input_files:
        print(f"\n--- Processing file: {input_file_path.name} ---")
        try:
            dataset = pd.read_csv(input_file_path, index_col=0)
            dataset.index = pd.to_datetime(dataset.index)
        except IOError as e:
            print(f'File not found: {input_file_path.name}. Skipping.')
            continue # Skip to the next file

        # Let us create our visualization class again.
        DataViz = VisualizeDataset(__file__)

        # Ensure there are enough data points and a valid time difference
        if len(dataset.index) < 2:
            print(f"Dataset {input_file_path.name} has less than 2 data points. Cannot determine sampling frequency. Skipping.")
            continue

        time_difference_microseconds = (dataset.index[1] - dataset.index[0]).microseconds
        if time_difference_microseconds == 0:
            print(f"Dataset {input_file_path.name} has zero time difference between first two points. Cannot determine sampling frequency. Skipping.")
            continue

        milliseconds_per_instance = time_difference_microseconds / 1000
        if milliseconds_per_instance == 0:
            print(f"Dataset {input_file_path.name} has effectively zero milliseconds per instance. Cannot determine sampling frequency. Skipping.")
            continue

        # Construct the base name for the output file
        base_output_name = input_file_path.name.replace('chapter3_result_final_', 'chapter4_result_')


        if FLAGS.mode == 'aggregation':
            # Chapter 4: Identifying aggregate attributes.

            # Set the window sizes to the number of instances representing 5 seconds, 30 seconds and 5 minutes
            window_sizes = [int(float(5000) / milliseconds_per_instance),
                            int(float(0.5 * 60000) / milliseconds_per_instance),
                            int(float(5 * 60000) / milliseconds_per_instance)]

            # please look in Chapter4 TemporalAbstraction.py to look for more aggregation methods or make your own.

            for ws in window_sizes:
                dataset = NumAbs.abstract_numerical(dataset, [ACC_X], ws, 'mean')
                dataset = NumAbs.abstract_numerical(dataset, [ACC_X], ws, 'std')

            DataViz.plot_dataset(dataset, [ACC_X, f'{ACC_X}_temp_mean', f'{ACC_X}_temp_std', 'label'],
                                 ['exact', 'like', 'like', 'like'], ['line', 'line', 'line', 'points'])
            output_file = DATA_PATH / f'{base_output_name.replace(".csv", "")}_aggregation.csv'
            dataset.to_csv(output_file)
            print(f"Results for {input_file_path.name} saved to: {output_file}")


        if FLAGS.mode == 'frequency':
            # Now we move to the frequency domain, with the same window size.

            fs = float(1000) / milliseconds_per_instance
            ws = int(float(10000) / milliseconds_per_instance)
            dataset = FreqAbs.abstract_frequency(dataset, [ACC_X], ws, fs)
            # Spectral analysis.
            DataViz.plot_dataset(dataset, [f'{ACC_X}_max_freq', f'{ACC_X}_freq_weighted', f'{ACC_X}_pse', 'label'],
                                 ['like', 'like', 'like', 'like'], ['line', 'line', 'line', 'points'])
            output_file = DATA_PATH / f'{base_output_name.replace(".csv", "")}_frequency.csv'
            dataset.to_csv(output_file)
            print(f"Results for {input_file_path.name} saved to: {output_file}")


        if FLAGS.mode == 'final':
            ws = int(float(0.5 * 60000) / milliseconds_per_instance)
            fs = float(1000) / milliseconds_per_instance

            # We need to get all numerical columns excluding 'label' for abstraction.
            # Filter to ensure only numeric and non-boolean columns are selected
            selected_predictor_cols = [c for c in dataset.columns
                                       if not ('label' in c) and
                                       pd.api.types.is_numeric_dtype(dataset[c]) and
                                       not pd.api.types.is_bool_dtype(dataset[c])]

            dataset = NumAbs.abstract_numerical(dataset, selected_predictor_cols, ws, 'mean')
            dataset = NumAbs.abstract_numerical(dataset, selected_predictor_cols, ws, 'std')
            # TODO: Add your own aggregation methods here

            # Adjusted plot_dataset to use appropriate columns from bouldering dataset.
            DataViz.plot_dataset(dataset, [ACC_X, GYR_X, MAG_X, LOC_HEIGHT, PCA_1, 'label'],
                                 ['like', 'like', 'like', 'like', 'like', 'like'],
                                 ['line', 'line', 'line', 'line', 'line', 'points'])

            CatAbs = CategoricalAbstraction()

            # Ensure 'label' column exists before abstracting categorical data
            if 'label' in dataset.columns:
                dataset = CatAbs.abstract_categorical(dataset, ['label'], ['like'], 0.03,
                                                      int(float(5 * 60000) / milliseconds_per_instance), 2)
            else:
                print(f"Warning: 'label' column not found in {input_file_path.name}. Skipping categorical abstraction.")


            periodic_predictor_cols = ['acc_X (m/s^2)', 'acc_Y (m/s^2)', 'acc_Z (m/s^2)',
                                       "gyr_X (rad/s)","gyr_Y (rad/s)","gyr_Z (rad/s)",
                                       "mag_X (µT)","mag_Y (µT)","mag_Z (µT)"]
            # 'loc_Height (m)' and 'loc_Velocity (m/s)' are typically not used for Fourier Transform in this context,
            # so they are excluded from periodic_predictor_cols to align with typical usage.

            # Filter periodic_predictor_cols to ensure they are numeric and exist
            periodic_measurements_for_freq = [col for col in periodic_predictor_cols
                                              if col in dataset.columns and pd.api.types.is_numeric_dtype(dataset[col])]

            if not periodic_measurements_for_freq:
                print(f"Warning: No valid numeric periodic measurements found in {input_file_path.name}. Skipping frequency abstraction.")
            else:
                dataset = FreqAbs.abstract_frequency(copy.deepcopy(dataset), periodic_measurements_for_freq,
                                                     int(float(10000) / milliseconds_per_instance), fs)

            # Now we only take a certain percentage of overlap in the windows, otherwise our training examples will be too much alike.

            # The percentage of overlap we allow
            window_overlap = 0.9
            skip_points = int((1 - window_overlap) * ws)

            # Ensure skip_points is at least 1 to avoid infinite loop or errors with iloc
            if skip_points == 0:
                print(f"Warning: Calculated skip_points is 0 for {input_file_path.name}. Setting to 1 to avoid errors in iloc.")
                skip_points = 1

            dataset = dataset.iloc[::skip_points, :]

            output_file = DATA_PATH / f'{base_output_name}'
            dataset.to_csv(output_file)
            print(f"Final results for {input_file_path.name} saved to: {output_file}")
    print("--- Total processing time: %s seconds ---" % (time.time() - start_time))


if __name__ == '__main__':
    # Command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', type=str, default='final',
                        help="Select what version to run: final, aggregation or freq \
                        'aggregation' studies the effect of several aggeregation methods \
                        'frequency' applies a Fast Fourier transformation to a single variable \
                        'final' is used for the next chapter ", choices=['aggregation', 'frequency', 'final'])

    FLAGS, unparsed = parser.parse_known_args()

    main()